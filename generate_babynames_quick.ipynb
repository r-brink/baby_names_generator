{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quick baby name generation\n",
    "\n",
    "This notebook loads a pretrained model. It is therefore not needed to wait for the model to fit. If you want to get more details about the data processing, transformation and fitting, I refer to the other notebook with suffix '_complete_'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import unidecode\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import load_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = load_model('./models/model.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('model_input/names.txt', 'r') as text:\n",
    "    list_of_names = text.read()\n",
    "\n",
    "names = list_of_names.splitlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_ = []\n",
    "\n",
    "for name in names:\n",
    "    x = name.split('-')\n",
    "    x = ''.join(x)\n",
    "    x = unidecode.unidecode(x)\n",
    "    x = str(x)+'.'\n",
    "    x = x.lower()\n",
    "    x = x.replace(\"'\", \"\")\n",
    "  \n",
    "    _.append(x)\n",
    "\n",
    "names = _"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~',\n",
    "    split='\\n')\n",
    "tokenizer.fit_on_texts(list_of_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "char_to_index = tokenizer.word_index\n",
    "index_to_char = dict((v, k) for k, v in char_to_index.items())\n",
    "\n",
    "char_to_index['.'] = 0\n",
    "index_to_char[0] = '.'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_char = len(max(names, key=len))\n",
    "m = len(names)\n",
    "char_dim = len(char_to_index)\n",
    "\n",
    "X = np.zeros((m, max_char, char_dim))\n",
    "Y = np.zeros((m, max_char, char_dim))\n",
    "\n",
    "for i in range(m):\n",
    "    name = list(names[i])\n",
    "    for j in range(len(name)):\n",
    "        X[i, j, char_to_index[name[j]]] = 1\n",
    "        if j < len(name)-1:\n",
    "            Y[i, j, char_to_index[name[j+1]]] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_name(model):\n",
    "    name = []\n",
    "    x = np.zeros((1, max_char, char_dim))\n",
    "    end = False\n",
    "    i = 0\n",
    "    \n",
    "    while end==False:\n",
    "        probs = list(model.predict(x)[0,i])\n",
    "        probs = probs / np.sum(probs)\n",
    "        index = np.random.choice(range(char_dim), p=probs)\n",
    "        if i == max_char-2:\n",
    "            character = '.'\n",
    "            end = True\n",
    "        else:\n",
    "            character = index_to_char[index]\n",
    "        name.append(character)\n",
    "        x[0, i+1, index] = 1\n",
    "        i += 1\n",
    "        if character == '.':\n",
    "            end = True\n",
    "    print(''.join(name))\n",
    "    \n",
    "    return ''.join(name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = './model_output/'\n",
    "filename = 'generated_names.txt'\n",
    "\n",
    "number_of_names = 10\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "if not os.path.isfile(f'{path}/{filename}'):\n",
    "    open(f'{path}/{filename}', 'w').close()\n",
    "\n",
    "with open(f'{path}/{filename}', 'a') as text:\n",
    "      \n",
    "    output = []\n",
    "    \n",
    "    for i in range(number_of_names):\n",
    "        x = str(make_name(model)[:-1]) + '\\n'\n",
    "        output.append(x)\n",
    "      \n",
    "    [text.write(x) for x in output]\n",
    "\n",
    "text.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load list of generated names\n",
    "with open(f'{path}/{filename}', 'r') as file:\n",
    "    gen_names = file.read()\n",
    "    gen_names = gen_names.splitlines()\n",
    "file.close()\n",
    "\n",
    "_ = []\n",
    "\n",
    "for names in gen_names:\n",
    "  _.append(names[:-1])\n",
    "\n",
    "gen_names = pl.Series(_)\n",
    "\n",
    "# Read original names\n",
    "with open(f'model_input/names.txt', 'r') as file:\n",
    "    original_names = file.read()\n",
    "    original_names = original_names.splitlines()\n",
    "file.close()\n",
    "\n",
    "_ = []\n",
    "\n",
    "for name in original_names:\n",
    "  _.append(name.lower())\n",
    "\n",
    "original_names = list(_)\n",
    "\n",
    "# Compare generated names with original names\n",
    "check_names = gen_names.is_in(original_names)\n",
    "name_existing = list(zip(list(gen_names), check_names.to_list()))\n",
    "\n",
    "for value in name_existing:\n",
    "  print(value)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "4f9aae5b6cf8db0a420f9377589481142c863fd46413551f815e8332cf444698"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}