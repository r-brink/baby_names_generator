{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('venv': venv)"
    },
    "interpreter": {
      "hash": "643501c3694d5809b6c22e57be5af8e366cd883da5385c57bb67a5965665f6f6"
    },
    "colab": {
      "name": "generate_babynames_complete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating baby names"
      ],
      "metadata": {
        "id": "WSWd7UaFLYl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "The architecture for the RNN is strongly inspired by the Pokemon Name Generator of Yan Gobeil [(Github link)](https://github.com/yangobeil/Pokemon-name-generator/blob/master/Generate%20Pok%C3%A9mon%20names.ipynb?utm_source=pocket_mylist). With a relatively simple model, Yan was able to produce interesting results. "
      ],
      "metadata": {
        "id": "0vaPhZxhLYl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing"
      ],
      "metadata": {
        "id": "VxypwOChLYl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with any data project, getting and wrangling the data is the most work. There are endless lists of popular first names over the years, but it is nearly impossible to download a single file with a total overview. In the Netherlands you can find some overview of baby names at the Sociale Verzekeringsbank. This is the instance that pays child benefit to parents. We can find an overview of children names here: [SVB kindernamen](https://www.svbkindernamen.nl/nl/kindernamen/index.htm). From the page we manually copy the table from 2020 and 2019 and transform it in a CSV file."
      ],
      "metadata": {
        "id": "rZdR1riWLYmA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Uncomment the following lines if you are running this notebook in Google Colab\n",
        "!pip install polars\n",
        "!pip install Unidecode"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting polars\n",
            "  Downloading polars-0.8.17-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 8.4 MB/s \n",
            "\u001b[?25hCollecting pyarrow>=4.0.*\n",
            "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.6 MB 126 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from polars) (1.19.5)\n",
            "Installing collected packages: pyarrow, polars\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "Successfully installed polars-0.8.17 pyarrow-5.0.0\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 9.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.2.0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "042OCFWwLYmA",
        "outputId": "63977ed0-7196-44d6-8531-2e6568c229fc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Use Polars instead of Pandas\n",
        "import polars as pl"
      ],
      "outputs": [],
      "metadata": {
        "id": "hk9WEXN-LYmB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "df_nl20 = pl.read_csv('data/NL_MEISJESNAMEN_2020.csv', encoding='utf8', sep=';', columns=['Naam', 'Aantal'], )\n",
        "print(df_nl20.head(5))\n",
        "print(f'There are: {len(df_nl20)} names in the dataframe')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "╭───────────┬────────╮\n",
            "│ Naam      ┆ Aantal │\n",
            "│ ---       ┆ ---    │\n",
            "│ str       ┆ i64    │\n",
            "╞═══════════╪════════╡\n",
            "│ \"Aaliyah\" ┆ 41     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Abby\"    ┆ 38     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Abigail\" ┆ 39     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Ada\"     ┆ 33     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Adriana\" ┆ 59     │\n",
            "╰───────────┴────────╯\n",
            "There are: 518 names in the dataframe\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIBBuIfYLYmC",
        "outputId": "c3f6e497-89f9-4523-f948-a612d9c22e9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "df_nl19 = pl.read_csv('data/NL_MEISJESNAMEN_2019.csv', encoding='utf8', sep=';', columns=['Naam', 'Aantal'], )\n",
        "print(df_nl19.head(5))\n",
        "print(f'There are: {len(df_nl19)} names in the dataframe')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "╭───────────┬────────╮\n",
            "│ Naam      ┆ Aantal │\n",
            "│ ---       ┆ ---    │\n",
            "│ str       ┆ i64    │\n",
            "╞═══════════╪════════╡\n",
            "│ \"Aaliyah\" ┆ 61     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aaltje\"  ┆ 28     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Abby\"    ┆ 36     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Abigail\" ┆ 44     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Ada\"     ┆ 34     │\n",
            "╰───────────┴────────╯\n",
            "There are: 525 names in the dataframe\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZcpAmf0LYmD",
        "outputId": "e4cf3f1b-baad-4901-e0ec-1dd2cc7ed233"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can already see that there is quite some overlap between the years. Let's create on list and see how big the overlap is."
      ],
      "metadata": {
        "id": "_DbAWFXNLYmD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Create one single dataframe with all names\n",
        "df_stacked = df_nl20.vstack(df_nl19)\n",
        "df_stacked['Naam'].is_duplicated().sum()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "932"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jTgX50fLYmF",
        "outputId": "aa6299eb-14f3-494e-e5d4-e91c943afed4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering we have a total of little more than a 1000 names, 932 duplicated values is.. quite a bit. It makes sense when we take into account that names with a frequency lower than 25 are left out because of privacy reasons."
      ],
      "metadata": {
        "id": "Vyer6kKRLYmG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Create a list with unique names and frequencies\n",
        "df_nl = df_stacked.groupby('Naam').agg([pl.sum('Aantal')])\n",
        "\n",
        "# Restoring the original column names\n",
        "df_nl.columns = ['Naam', 'Aantal']\n",
        "\n",
        "print(df_nl.head())\n",
        "print(f'There are: {len(df_nl)} names in the dataframe')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "╭───────────┬────────╮\n",
            "│ Naam      ┆ Aantal │\n",
            "│ ---       ┆ ---    │\n",
            "│ str       ┆ i64    │\n",
            "╞═══════════╪════════╡\n",
            "│ \"Jessie\"  ┆ 132    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Jaylinn\" ┆ 172    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Philou\"  ┆ 261    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Lua\"     ┆ 29     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Quinn\"   ┆ 86     │\n",
            "╰───────────┴────────╯\n",
            "There are: 577 names in the dataframe\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlufbsdGLYmG",
        "outputId": "de0f69ad-a292-4691-b07e-5b03aad81288"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# Showing a small sample from the dataset\n",
        "print(df_nl.sort('Aantal', reverse=True).sample(frac=0.01))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "╭──────────┬────────╮\n",
            "│ Naam     ┆ Aantal │\n",
            "│ ---      ┆ ---    │\n",
            "│ str      ┆ i64    │\n",
            "╞══════════╪════════╡\n",
            "│ \"Joëlle\" ┆ 107    │\n",
            "├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Layla\"  ┆ 104    │\n",
            "├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Myla\"   ┆ 71     │\n",
            "├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Jinthe\" ┆ 172    │\n",
            "├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Ivy\"    ┆ 424    │\n",
            "╰──────────┴────────╯\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6_awR_0LYmH",
        "outputId": "d42c9490-f8a2-49e7-ee9d-f8079ae1c0e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "577 names are not a lot for a neural network. It would be very unlikely that we will get useful results out of it. After some searching I also found a list of the Nederlandse Voornamenbank (maintained by the [Meertens Instituut](https://www.meertens.knaw.nl/nvb/)). Although they haven't replied to my inquiries to get direct access to their database, I found another list at [Naamkunde.net](http://www.naamkunde.net/). Let's process this data as well.\n",
        "\n",
        "We take all names from 1995 to 2006. Although they are old, we concluded earlier that there are not many new names introduced every year."
      ],
      "metadata": {
        "id": "oBm8qY49LYmH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "df_nl9506 = pl.read_csv('data/NL_MEISJESNAMEN_19952006.csv', encoding='utf8', sep=';', columns=['Naam', 'Aantal'])\n",
        "print(df_nl9506.head(5))\n",
        "print(f'There are {len(df_nl9506)} names in the dataframe')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "╭───────────────┬────────╮\n",
            "│ Naam          ┆ Aantal │\n",
            "│ ---           ┆ ---    │\n",
            "│ str           ┆ i64    │\n",
            "╞═══════════════╪════════╡\n",
            "│ \"Aafje (V)\"   ┆ 131    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aafke (V)\"   ┆ 744    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aagje (V)\"   ┆ 272    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aagtje (V)\"  ┆ 29     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aaliyah (V)\" ┆ 257    │\n",
            "╰───────────────┴────────╯\n",
            "There are 5343 names in the dataframe\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaPO3VbXLYmI",
        "outputId": "f981088a-3e5b-4abe-99dd-eed78960128f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# To prevent the model from learning that names contain (V) we remove these from the names\n",
        "df_nl9506['Naam'] = [(i.split(' ')[0]) for i in df_nl9506['Naam']]\n",
        "print(df_nl9506.head(5))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "╭───────────┬────────╮\n",
            "│ Naam      ┆ Aantal │\n",
            "│ ---       ┆ ---    │\n",
            "│ str       ┆ i64    │\n",
            "╞═══════════╪════════╡\n",
            "│ \"Aafje\"   ┆ 131    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aafke\"   ┆ 744    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aagje\"   ┆ 272    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aagtje\"  ┆ 29     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Aaliyah\" ┆ 257    │\n",
            "╰───────────┴────────╯\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b9b6z3ALYmI",
        "outputId": "c2a82911-9ef7-4bc6-a621-dc8e81a5a55d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "df_nl = df_nl.vstack(df_nl9506)"
      ],
      "outputs": [],
      "metadata": {
        "id": "eWOvwRJWLYmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# Creating one single file again\n",
        "df_nl = df_nl.groupby('Naam').agg([pl.sum('Aantal')])"
      ],
      "outputs": [],
      "metadata": {
        "id": "HkRvcENDLYmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# 10 most popular names from our datasets\n",
        "print(df_nl.sort('Aantal_sum', reverse=True)[:10])\n",
        "print(f'There are {len(df_nl)} names in the dataframe')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (10, 2)\n",
            "╭───────────┬────────────╮\n",
            "│ Naam      ┆ Aantal_sum │\n",
            "│ ---       ┆ ---        │\n",
            "│ str       ┆ i64        │\n",
            "╞═══════════╪════════════╡\n",
            "│ \"Sanne\"   ┆ 22651      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Maria\"   ┆ 21899      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Laura\"   ┆ 20961      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Anne\"    ┆ 20823      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ ...       ┆ ...        │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Lisa\"    ┆ 18078      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Anna\"    ┆ 18030      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Iris\"    ┆ 17285      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Johanna\" ┆ 16975      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Eva\"     ┆ 14845      │\n",
            "╰───────────┴────────────╯\n",
            "There are 5413 names in the dataframe\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uShpCB1KLYmJ",
        "outputId": "abee084b-3078-448a-f067-f2a636b231d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5400 names is a lot better than before. After experimenting I found that this is still not enough. Fortunately, Belgium has similar names. Some web searching shows that the Belgium government actually publishes a list of first names. It can be found [here](https://statbel.fgov.be/nl/themas/bevolking/namen-en-voornamen). This list is immense and we can download all the names from 1995 to 2020. I have chosen here to also include the Wallonian names, even though these seem more French in general. "
      ],
      "metadata": {
        "id": "zqRV1ZECLYmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "df_be = pl.read_csv('data/BE_MEISJESNAMEN_19952020.csv', encoding='utf8', sep=';', columns=['Naam', 'Aantal'])\n",
        "print(df_be.head(5))\n",
        "print(f'There are {len(df_be)} names in the dataframe')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "╭──────────┬────────╮\n",
            "│ Naam     ┆ Aantal │\n",
            "│ ---      ┆ ---    │\n",
            "│ str      ┆ i64    │\n",
            "╞══════════╪════════╡\n",
            "│ \"Emma\"   ┆ 15779  │\n",
            "├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Laura\"  ┆ 15260  │\n",
            "├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Marie\"  ┆ 13922  │\n",
            "├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Louise\" ┆ 12334  │\n",
            "├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Sarah\"  ┆ 11973  │\n",
            "╰──────────┴────────╯\n",
            "There are 12799 names in the dataframe\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_lcQoekLYmK",
        "outputId": "c08fcd09-6e50-42a2-f3eb-4864d4977411"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# Checking if we need to clean the list and remove duplicates\n",
        "df_be['Naam'].is_duplicated().sum()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLmF65hHLYmL",
        "outputId": "36335374-5e64-4932-e9a6-55644f5032b3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# Creating one overview \n",
        "df = df_be.vstack(df_nl)\n",
        "df = df.groupby('Naam').agg([pl.sum('Aantal')])\n",
        "df.columns = ['Naam', 'Aantal']"
      ],
      "outputs": [],
      "metadata": {
        "id": "2znR0ZocLYmL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "print(df.sort(by='Aantal', reverse=True).sample(frac=0.0005))\n",
        "print(f'There are {len(df)} names in the dataframe')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (7, 2)\n",
            "╭────────────┬────────╮\n",
            "│ Naam       ┆ Aantal │\n",
            "│ ---        ┆ ---    │\n",
            "│ str        ┆ i64    │\n",
            "╞════════════╪════════╡\n",
            "│ \"Tya\"      ┆ 72     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Lauri\"    ┆ 205    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Linna\"    ┆ 10     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Djara\"    ┆ 7      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Trish\"    ┆ 9      │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Eleanore\" ┆ 60     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
            "│ \"Noä\"      ┆ 16     │\n",
            "╰────────────┴────────╯\n",
            "There are 14371 names in the dataframe\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3s7eVm2LYmL",
        "outputId": "71a66c42-3c2b-41ac-928f-d3b2ea912e9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have also including a large dataset with names from the USA to experiment with. This set can be found [here](https://www.ssa.gov/oact/babynames/limits.html). More names give more interesting results from the neural network that we are going to use. You can choose to comment out the following cell to leave these names out of the model."
      ],
      "metadata": {
        "id": "Z1NrGvz0LYmM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "# Processing the dataset in one cell to create a single file to work with again.\n",
        "df_int = pl.read_csv('data/INT_MEISJESNAMEN_2020.csv', encoding='utf8', sep=';', columns=['Naam', 'Aantal'])\n",
        "\n",
        "df = df.vstack(df_int)\n",
        "\n",
        "df = df.groupby('Naam').agg([pl.sum('Aantal')])\n",
        "\n",
        "print(df.head(5))\n",
        "print(f'There are {len(df)} names in the dataframe')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "╭────────────┬────────────╮\n",
            "│ Naam       ┆ Aantal_sum │\n",
            "│ ---        ┆ ---        │\n",
            "│ str        ┆ i64        │\n",
            "╞════════════╪════════════╡\n",
            "│ \"Legaciee\" ┆ 5          │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Maryjo\"   ┆ 11         │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Seriya\"   ┆ 6          │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Shanen\"   ┆ 5          │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ \"Serife\"   ┆ 149        │\n",
            "╰────────────┴────────────╯\n",
            "There are 26754 names in the dataframe\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI_K0buwLYmM",
        "outputId": "14876247-e111-40e9-f130-cb10dd914a20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "# The neural network only needs the actual names. The frequencies can be used to do some data analyses on.\n",
        "names = []\n",
        "\n",
        "for name in df['Naam']:\n",
        "    names.append(name)\n",
        "    \n",
        "len(names)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26754"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxopqqbcLYmN",
        "outputId": "5418f84c-daec-4d7e-9bba-239d7613d062"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "# We write the list of names to a text file, so that we don't have to do all the every time we want to run the model.\n",
        "with open('model_input/names.txt', 'w+') as f:\n",
        "      \n",
        "    # write elements of list\n",
        "    for items in names:\n",
        "        f.write('%s\\n' %items)\n",
        "\n",
        "    f.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "o-CJYSRJLYmN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "df.to_csv('data/all_names.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "wxxCAJUmLYmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are done with data processing. By merging several files with first names we have created a final list of 26754 names. The final list contains North American, Dutch and French names. Interesting mix of inputs for the model!"
      ],
      "metadata": {
        "id": "uPAKIYgxLYmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data transformation"
      ],
      "metadata": {
        "id": "CyAM0ct_LYmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all the names in one file we still have to do some transformations to make sure that the model is able to process the names. "
      ],
      "metadata": {
        "id": "Q7M74MBvLYmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "# Importing packages for the transformations\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import unidecode"
      ],
      "outputs": [],
      "metadata": {
        "id": "o7xnwjY9LYmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "# Loading the text set to train the Tokenizer and allowing to skip the processing steps\n",
        "with open('model_input/names.txt', 'r') as text:\n",
        "    list_of_names = text.read()\n",
        "\n",
        "list_of_names[:50]\n",
        "\n",
        "names = list_of_names.splitlines()"
      ],
      "outputs": [],
      "metadata": {
        "id": "CyTCAfx2LYmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "# Using a temporary variable\n",
        "_ = []\n",
        "\n",
        "# Removing hyphens and other things like umlauts\n",
        "for name in names:\n",
        "    # split on hyphens\n",
        "    x = name.split('-')\n",
        "    x = ''.join(x)\n",
        "    # normalise all text to plain letters\n",
        "    x = unidecode.unidecode(x)\n",
        "    # add dot to indicate end of name for model\n",
        "    x = str(x)+'.'\n",
        "    # lower case all letters for consistency\n",
        "    x = x.lower()\n",
        "    # remove apostrophes\n",
        "    x = x.replace(\"'\", \"\")\n",
        "  \n",
        "    _.append(x)\n",
        "\n",
        "# Reassigning the cleaned list to the names variable\n",
        "names = _\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "XVXdJd5ILYmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "# Define tokenizer to create mapping of all characters\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~',\n",
        "    split='\\n')"
      ],
      "outputs": [],
      "metadata": {
        "id": "LqRvJU_MLYmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "# Fitting the tokenizer on the original name list, without cleaning, to include all possible charachters \n",
        "tokenizer.fit_on_texts(list_of_names)"
      ],
      "outputs": [],
      "metadata": {
        "id": "oMfkEi6VLYmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "# Creating mappings for the characters\n",
        "char_to_index = tokenizer.word_index\n",
        "index_to_char = dict((v, k) for k, v in char_to_index.items())"
      ],
      "outputs": [],
      "metadata": {
        "id": "mWrMWeuQLYmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "# Adding a dot to both dictionaries, so the model can use it when generating names and knows when to move on\n",
        "char_to_index['.'] = 0\n",
        "index_to_char[0] = '.'\n",
        "\n",
        "print(char_to_index)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 1, 'e': 2, 'i': 3, 'n': 4, 'l': 5, 'r': 6, 'y': 7, 's': 8, 'h': 9, 'm': 10, 'o': 11, 't': 12, 'd': 13, 'k': 14, 'u': 15, 'c': 16, 'j': 17, 'b': 18, 'z': 19, 'v': 20, 'g': 21, 'f': 22, 'p': 23, 'é': 24, 'w': 25, 'x': 26, 'ï': 27, 'ë': 28, 'q': 29, 'ü': 30, 'è': 31, 'â': 32, 'ş': 33, 'ç': 34, 'í': 35, 'ö': 36, 'ó': 37, \"'\": 38, 'ä': 39, 'á': 40, 'î': 41, 'ı': 42, 'ğ': 43, 'ÿ': 44, 'û': 45, 'i̇': 46, 'ù': 47, 'ê': 48, 'ú': 49, 'š': 50, 'ĝ': 51, 'å': 52, 'æ': 53, 'à': 54, '.': 0}\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk-bclWILYmQ",
        "outputId": "972b24af-882a-420d-a10b-7d4b117de11d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "# Maximum number of characters in names. These are number of time steps used in the RNN model\n",
        "max_char = len(max(names, key=len))\n",
        "\n",
        "# Amount of names that are available\n",
        "m = len(names)\n",
        "\n",
        "# Number of potential characters\n",
        "char_dim = len(char_to_index)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mfJsAjh1LYmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "# Converting the list of names to a training dataset. This creates a matrix for each of the available names in 'm'.\n",
        "X = np.zeros((m, max_char, char_dim))\n",
        "Y = np.zeros((m, max_char, char_dim))\n",
        "\n",
        "for i in range(m):\n",
        "    name = list(names[i])\n",
        "    for j in range(len(name)):\n",
        "        X[i, j, char_to_index[name[j]]] = 1\n",
        "        if j < len(name)-1:\n",
        "            Y[i, j, char_to_index[name[j+1]]] = 1"
      ],
      "outputs": [],
      "metadata": {
        "id": "FaS8MzKXLYmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating names with an Recurrent Neural Network (RNN)"
      ],
      "metadata": {
        "id": "ghMSVKvmLYmR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "import os\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.callbacks import LambdaCallback"
      ],
      "outputs": [],
      "metadata": {
        "id": "U9XqzUndLYmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the explanation of Yan Goebel on how the names are generated (from his original notebook): _\"The idea is to input empty characters to the trained network and use the output of the first time step as a probability distribution for the first letter of the name. We then use this distribution to decide randomly the first character, record it and update the input to pass this character as an input for the second time step. This is continued for the following time steps to create a name._\n",
        "\n",
        "_This is where using a '.' at the end of each name becomes important, because we stop the procedure once we get a '.' as an output, meaning that the generated name is done. Also if we reach the length of the largest name in the training set we put a '.' and end the procedure.\"_"
      ],
      "metadata": {
        "id": "Gd3VdQXSLYmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "def make_name(model):\n",
        "    name = []\n",
        "    x = np.zeros((1, max_char, char_dim))\n",
        "    end = False\n",
        "    i = 0\n",
        "    \n",
        "    while end==False:\n",
        "        probs = list(model.predict(x)[0,i])\n",
        "        probs = probs / np.sum(probs)\n",
        "        index = np.random.choice(range(char_dim), p=probs)\n",
        "        if i == max_char-2:\n",
        "            character = '.'\n",
        "            end = True\n",
        "        else:\n",
        "            character = index_to_char[index]\n",
        "        name.append(character)\n",
        "        x[0, i+1, index] = 1\n",
        "        i += 1\n",
        "        if character == '.':\n",
        "            end = True\n",
        "    print(''.join(name))\n",
        "    return ''.join(name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ngobzoUjLYmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To monitor the model during training the following function is defined. After every 25 epochs we print 5 results to see what names are generated."
      ],
      "metadata": {
        "id": "XJnicHJVLYmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "def generate_name_loop(epoch, _):\n",
        "    if epoch % 25 == 0:\n",
        "        \n",
        "        print(f'Names generated after epoch {epoch}')\n",
        "\n",
        "        for i in range(5):\n",
        "            make_name(model)\n",
        "        \n",
        "        print()"
      ],
      "outputs": [],
      "metadata": {
        "id": "QFuSCXGBLYmT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "# Convert the function to be able to use it as callback function. \n",
        "name_generator = LambdaCallback(on_epoch_end = generate_name_loop)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VnzBHkU3LYmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Yan Gobeil: _\"In the case of interest here we only consider one layer of recurrence, which we take to be LSTM with 128 units. We return the output of this layer and use it into a fully connected dense layer that converts the result of the LSTM layer into a vector of size char_dim using a softmax activation. We use categorical cross entropy as a cost function because of the softmax result and use Adam optimization. There is not really any useful metric to judge if the model does good so we will mostly just look at the results.\"_\n"
      ],
      "metadata": {
        "id": "QbrMIWsnLYmT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "source": [
        "# \n",
        "# Neural network architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(max_char, char_dim), return_sequences=True))\n",
        "model.add(LSTM(128, input_shape=(max_char, char_dim), return_sequences=True))\n",
        "model.add(Dense(char_dim, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "model.fit(X, Y, epochs=276, batch_size=128, callbacks=[name_generator], verbose=0)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Names generated after epoch 0\n",
            "rsælkèhkdagorambš.\n",
            "ocabémmašáğ.\n",
            "æznífæê.\n",
            "byl.\n",
            "oaá.\n",
            "\n",
            "Names generated after epoch 25\n",
            "annoh.\n",
            "adeĝc.\n",
            "eatliise.\n",
            "elunrhsyehbhè.\n",
            "ilièalåh.\n",
            "\n",
            "Names generated after epoch 50\n",
            "digine.\n",
            "ùeæ.\n",
            "apíu.\n",
            "abea.\n",
            "avàma.\n",
            "\n",
            "Names generated after epoch 75\n",
            "ellia.\n",
            "amina.\n",
            "aflyve.\n",
            "anneki.\n",
            "inyen.\n",
            "\n",
            "Names generated after epoch 100\n",
            "avyha.\n",
            "autun.\n",
            "elamari.\n",
            "enicaá.\n",
            "o.\n",
            "\n",
            "Names generated after epoch 125\n",
            "elynn.\n",
            "a.\n",
            "rane.\n",
            "apena.\n",
            "yaei.\n",
            "\n",
            "Names generated after epoch 150\n",
            "àel.\n",
            "annet'.\n",
            "akela.\n",
            "herila.\n",
            "adce.\n",
            "\n",
            "Names generated after epoch 175\n",
            "oûna.\n",
            "aveenn.\n",
            "aiden.\n",
            "amiyah.\n",
            "iû.\n",
            "\n",
            "Names generated after epoch 200\n",
            "ayelint.\n",
            "aliyai.\n",
            "olla.\n",
            "ayaè.\n",
            "acilea.\n",
            "\n",
            "Names generated after epoch 225\n",
            "oraya.\n",
            "anyelis.\n",
            "amary.\n",
            "alsinah.\n",
            "anye.\n",
            "\n",
            "Names generated after epoch 250\n",
            "icca.\n",
            "alea.\n",
            "emsia.\n",
            "avia.\n",
            "amtin.\n",
            "\n",
            "Names generated after epoch 275\n",
            "hilalia.\n",
            "aålyaz.\n",
            "amanie.\n",
            "aneska.\n",
            "amey.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb270000dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSWQrOFyLYmT",
        "outputId": "1099fdfc-c3ad-4d65-b86c-ad9d71804c95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is clear that the model is making significant improvements every 25 epochs. Where the first generated names are jibberish (e.g.: '*ocabémmašáğ*'), we see that some potential names are formed in later epochs. During experimentation with the model it seems that it starts to overfit after 200 epochs. Considering our goal this might actually be good. Remembering our data processing steps, we already saw that names don't change that much over time. But we are still looking for something novel, so there are limits to the overfitting. Looking at the results in the latest printed epoch, the names seem novel ('Amey' or 'Aneska')."
      ],
      "metadata": {
        "id": "FSwIgqoDLYmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save generated names and the model"
      ],
      "metadata": {
        "id": "90c6ooEPLYmU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "source": [
        "# Set path and filename to store results\n",
        "path = './model_output/'\n",
        "filename = 'generated_names.txt'\n",
        "\n",
        "# Set number of names to be generated by model\n",
        "number_of_names = 25\n",
        "\n",
        "# Create directory to store names if not existing\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "# Create file to store names if not existing\n",
        "if not os.path.isfile(f'{path}/{filename}'):\n",
        "    open(f'{path}/{filename}', 'w').close()\n",
        "\n",
        "# Append the number of names to the file so previous names don't get overwritten\n",
        "with open(f'{path}/{filename}', 'a') as text:\n",
        "      \n",
        "    output = []\n",
        "    \n",
        "    for i in range(number_of_names):\n",
        "        # Removing the dot at the end and printing names on a new line\n",
        "        x = str(make_name(model)[:-1]) + '\\n'\n",
        "        output.append(x)\n",
        "      \n",
        "    [text.write(x) for x in output]\n",
        "\n",
        "text.close()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "olnee.\n",
            "ably.\n",
            "aizja.\n",
            "asziá.\n",
            "anyana.\n",
            "ayva.\n",
            "ahara.\n",
            "eorna.\n",
            "ezly.\n",
            "aniyah.\n",
            "jose.\n",
            "anylahy.\n",
            "anlee.\n",
            "odden.\n",
            "efly.\n",
            "irke.\n",
            "untiah.\n",
            "hajve.\n",
            "orra.\n",
            "amtin.\n",
            "arlyn.\n",
            "iulica.\n",
            "alaita.\n",
            "ada.\n",
            "evrie.\n"
          ]
        }
      ],
      "metadata": {
        "id": "3RluVdAxLYmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f53322-7f26-41c3-c157-7d9c26a165de"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "source": [
        "# Saving the model so that we can use it later to quickly generate new names\n",
        "model.save(f'./model/model.h5')  # creates a HDF5 file"
      ],
      "outputs": [],
      "metadata": {
        "id": "uxinTKxVLYmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Names, names, names"
      ],
      "metadata": {
        "id": "rxXMrOZ8LYmV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "source": [
        "# Loading the list of generated names\n",
        "with open(f'{path}/{filename}', 'r') as file:\n",
        "    gen_names = file.read()\n",
        "    gen_names = gen_names.splitlines()\n",
        "file.close()\n",
        "\n",
        "gen_names = pl.Series(gen_names)"
      ],
      "outputs": [],
      "metadata": {
        "id": "DdVn868XLYmW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "source": [
        "# Loading the list of original existing names \n",
        "with open(f'model_input/names.txt', 'r') as file:\n",
        "    original_names = file.read()\n",
        "    original_names = original_names.splitlines()\n",
        "file.close()\n",
        "\n",
        "_ = []\n",
        "\n",
        "for name in original_names:\n",
        "  _.append(name.lower())\n",
        "\n",
        "original_names = list(_)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mCkg4GemWWE8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "source": [
        "# Create a list of generated names and indicator if the name was in the original list\n",
        "# True means the name is already present in the original list. False means the name is 'new'\n",
        "check_names = gen_names.is_in(original_names)\n",
        "name_existing = list(zip(list(gen_names), check_names.to_list()))\n",
        "\n",
        "for value in name_existing:\n",
        "  print(value)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('riebe', False)\n",
            "('eoma', False)\n",
            "('ocheda', False)\n",
            "('ovee', True)\n",
            "('ola', True)\n",
            "('ola', True)\n",
            "('ollil', False)\n",
            "('ilou', False)\n",
            "('vila', False)\n",
            "('aleigh', True)\n",
            "('enika', False)\n",
            "('ionna', True)\n",
            "('ashana', True)\n",
            "('eliyah', True)\n",
            "('emminy', False)\n",
            "('aira', True)\n",
            "('aviana', True)\n",
            "('mou', False)\n",
            "('ylia', False)\n",
            "('ivere', False)\n",
            "('avia', True)\n",
            "('rielle', True)\n",
            "('ilayqu', False)\n",
            "('ambelle', False)\n",
            "('ocheda', False)\n",
            "('onsler', False)\n",
            "('akonamo', False)\n",
            "('ariah', True)\n",
            "('ada', True)\n",
            "('anoepa', False)\n",
            "('adeni', False)\n",
            "('armwi', False)\n",
            "('amber', True)\n",
            "('orianna', True)\n",
            "('amifje', False)\n",
            "('akosia', False)\n",
            "('uztar', False)\n",
            "('yanna', True)\n",
            "('oureyda', False)\n",
            "('utthel', False)\n",
            "('adia', True)\n",
            "('ami', True)\n",
            "('amara', True)\n",
            "('quer', False)\n",
            "('amilah', True)\n",
            "('amoni', True)\n",
            "('avani', True)\n",
            "('egritte', False)\n",
            "('ayen', True)\n",
            "('hija', False)\n",
            "('olnee', False)\n",
            "('ably', False)\n",
            "('aizja', False)\n",
            "('asziá', False)\n",
            "('anyana', False)\n",
            "('ayva', True)\n",
            "('ahara', False)\n",
            "('eorna', False)\n",
            "('ezly', False)\n",
            "('aniyah', True)\n",
            "('jose', True)\n",
            "('anylahy', False)\n",
            "('anlee', False)\n",
            "('odden', False)\n",
            "('efly', False)\n",
            "('irke', False)\n",
            "('untiah', False)\n",
            "('hajve', False)\n",
            "('orra', False)\n",
            "('amtin', False)\n",
            "('arlyn', True)\n",
            "('iulica', False)\n",
            "('alaita', False)\n",
            "('ada', True)\n",
            "('evrie', False)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqHYD3duXgJT",
        "outputId": "0f1823ae-1f28-4697-a786-72bb0f1e20c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final remarks\n",
        "It is interesting to find out that it is relatively easy to train an RNN to generate names. Fitting the model several times and generating names gave some interesting results. This use case shows the potential to offer an tool that generates outside-the-box content. I am convinced that this solution might name give someone his (if trained)/her name in the future. "
      ],
      "metadata": {}
    }
  ]
}